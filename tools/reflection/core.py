"""ë¦¬í”Œë ‰ì…˜ í•µì‹¬ ë¡œì§ ëª¨ë“ˆ. Reflection core logic module."""

from __future__ import annotations

from collections import defaultdict, deque
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Mapping, MutableMapping, Sequence

from .config import ReflectionConfig
from .exceptions import ReflectionError, TaskValidationError

TaskMapping = Mapping[str, Any]
TaskSequence = Sequence[TaskMapping]
CyclePaths = List[List[str]]


@dataclass(slots=True)
class ReflectionResult:
    """ë¦¬í”Œë ‰ì…˜ ì‹¤í–‰ ê²°ê³¼ ëª¨ë¸. Reflection execution result model."""

    tasks: List[Dict[str, Any]]
    meta: Dict[str, Any]
    cycles: List[List[str]]
    topo_order: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """JSON ì§ë ¬í™”ìš© ë”•ì…”ë„ˆë¦¬. Dictionary representation for JSON."""

        return {"tasks": self.tasks, "meta": self.meta}


class TaskReflector:
    """íƒœìŠ¤í¬ ë¦¬í”Œë ‰ì…˜ ì—”ì§„. Task reflection engine."""

    def __init__(self, config: ReflectionConfig | None = None):
        self.config = config or ReflectionConfig()

    def load_tasks(self, path: Path | str) -> Dict[str, Any]:
        """JSON íƒœìŠ¤í¬ íŒŒì¼ ë¡œë“œ. Load JSON task file."""

        try:
            import json

            with Path(path).open("r", encoding="utf-8") as handle:
                data = json.load(handle)
        except FileNotFoundError as exc:  # pragma: no cover - ì§ì ‘ í™•ì¸ ìš©ì´
            message = f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}"
            raise ReflectionError(message) from exc
        except OSError as exc:  # pragma: no cover - í™˜ê²½ ë¬¸ì œ ë³´ê³ 
            message = f"ì…ë ¥ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {exc}"
            raise ReflectionError(message) from exc
        except ValueError as exc:
            message = f"ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {exc}"
            raise TaskValidationError(message) from exc
        if not isinstance(data, MutableMapping):
            raise TaskValidationError(
                "ìµœìƒìœ„ ë°ì´í„°ëŠ” ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤. Root data must be object."
            )
        return dict(data)

    def detect_cycles(self, tasks: TaskSequence) -> CyclePaths:
        """ìˆœí™˜ ì˜ì¡´ì„± íƒì§€. Detect cyclic dependencies."""

        graph: Dict[str, List[str]] = {}
        for task in tasks:
            task_id = str(task.get("id"))
            if not task_id:
                raise TaskValidationError(
                    "ëª¨ë“  íƒœìŠ¤í¬ëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤. Every task requires id."
                )
            graph[task_id] = [str(dep) for dep in task.get("deps", [])]

        cycles: List[List[str]] = []
        visited: set[str] = set()
        stack: set[str] = set()
        path: List[str] = []

        def dfs(node: str) -> None:
            if node in stack:
                idx = path.index(node)
                cycles.append(path[idx:] + [node])
                return
            if node in visited:
                return
            visited.add(node)
            stack.add(node)
            path.append(node)
            for neighbor in graph.get(node, []):
                dfs(neighbor)
            stack.remove(node)
            path.pop()

        for task_id in graph:
            if task_id not in visited:
                dfs(task_id)
        return cycles

    def topological_sort(self, tasks: TaskSequence) -> List[str]:
        """í† í´ë¡œì§€ ì •ë ¬ ìˆ˜í–‰. Perform topological sort."""

        in_degree: Dict[str, int] = {}
        graph: Dict[str, List[str]] = {}
        for task in tasks:
            task_id = str(task.get("id"))
            if not task_id:
                raise TaskValidationError(
                    "ëª¨ë“  íƒœìŠ¤í¬ëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤. Every task requires id."
                )
            deps = [str(dep) for dep in task.get("deps", [])]
            graph[task_id] = deps
            in_degree[task_id] = len(deps)
        zero_degree: List[str] = []
        for task_id, degree in in_degree.items():
            if degree == 0:
                zero_degree.append(task_id)
        queue = deque(zero_degree)
        order: List[str] = []
        while queue:
            current = queue.popleft()
            order.append(current)
            for task_id, deps in graph.items():
                if current in deps:
                    in_degree[task_id] -= 1
                    if in_degree[task_id] == 0:
                        queue.append(task_id)
        return order

    def calculate_complexity(
        self,
        task: TaskMapping,
        all_tasks: TaskSequence,
    ) -> float:
        """ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°. Calculate complexity score."""

        task_type = str(task.get("type", "code"))
        base_complexity = self.config.type_complexity.get(task_type, 1.0)
        deps = task.get("deps", [])
        deps_count = len(deps) if isinstance(deps, Sequence) else 0
        dependents = 0
        for candidate in all_tasks:
            deps = candidate.get("deps", [])
            if task.get("id") in deps:
                dependents += 1
        title = str(task.get("title", "")).lower()
        bonus = 0.0
        for keyword, weight in self.config.title_bonus.items():
            if keyword in title:
                bonus += weight
        components = (
            base_complexity,
            0.2 * deps_count,
            0.1 * dependents,
            bonus,
        )
        total = sum(components)
        bounded = max(self.config.clamp_min, min(self.config.clamp_max, total))
        return round(bounded, 2)

    def reflect(self, tasks_data: TaskMapping) -> ReflectionResult:
        """íƒœìŠ¤í¬ ë°ì´í„°ë¥¼ ë¦¬í”Œë ‰ì…˜. Reflect task data structure."""

        tasks = tasks_data.get("tasks")
        if not isinstance(tasks, Sequence):
            raise TaskValidationError(
                "'tasks' í‚¤ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤. 'tasks' must be list."
            )
        normalized_tasks = [dict(task) for task in tasks]
        cycles = self.detect_cycles(normalized_tasks)
        topo_order = self.topological_sort(normalized_tasks)
        all_ids = [str(task.get("id")) for task in normalized_tasks]
        missing_ids: List[str] = []
        for task_id in all_ids:
            if task_id not in topo_order:
                missing_ids.append(task_id)
        ordered_ids = topo_order + missing_ids
        reflected_tasks: List[Dict[str, Any]] = []
        for index, task_id in enumerate(ordered_ids):
            task = None
            for candidate in normalized_tasks:
                if candidate.get("id") == task_id:
                    task = candidate
                    break
            if task is None:
                continue
            complexity = self.calculate_complexity(task, normalized_tasks)
            reflected_task = dict(task)
            reflected_task["order"] = index
            reflected_task["complexity"] = complexity
            reflected_tasks.append(reflected_task)
        meta = dict(tasks_data.get("meta", {}))
        meta["reflected_at"] = datetime.now().isoformat()
        meta["topo_order"] = topo_order
        meta["cycles_detected"] = len(cycles)
        if missing_ids:
            meta["unordered_tasks"] = missing_ids
        return ReflectionResult(reflected_tasks, meta, cycles, topo_order)

    def build_report(
        self,
        original_data: Mapping[str, Any],
        reflection: ReflectionResult,
    ) -> str:
        """ë¦¬í”Œë ‰ì…˜ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±. Generate reflection report text."""

        tasks = reflection.tasks
        total_tasks = len(tasks)
        complexities = [task["complexity"] for task in tasks]
        avg_complexity = 0.0
        if total_tasks:
            avg_complexity = sum(complexities) / total_tasks
        max_complexity = max(complexities, default=0)
        min_complexity = min(complexities, default=0)
        type_stats: Dict[str, List[float]] = defaultdict(list)
        module_stats: Dict[str, List[float]] = defaultdict(list)
        for task in tasks:
            task_type = str(task.get("type", "unknown"))
            module = str(task.get("module", "unknown"))
            type_stats[task_type].append(task["complexity"])
            module_stats[module].append(task["complexity"])
        source_meta = original_data.get("meta", {})
        source_value = source_meta.get("source", "N/A")
        report_lines = [
            "# íƒœìŠ¤í¬ ë¦¬í”Œë ‰ì…˜ ë¦¬í¬íŠ¸",
            "",
            "## ğŸ“Š **ë¦¬í”Œë ‰ì…˜ ìš”ì•½**",
            f"- **ìƒì„±ì¼**: {reflection.meta.get('reflected_at', 'N/A')}",
            f"- **ì›ë³¸ íŒŒì¼**: {source_value}",
            f"- **íƒœìŠ¤í¬ ìˆ˜**: {total_tasks}ê°œ",
            f"- **ìˆœí™˜ ì˜ì¡´ì„±**: {reflection.meta.get('cycles_detected', 0)}ê°œ ë°œê²¬",
            "",
            "## ğŸ“ˆ **ë³µì¡ë„ í†µê³„**",
            f"- **í‰ê·  ë³µì¡ë„**: {avg_complexity:.2f}",
            f"- **ìµœëŒ€ ë³µì¡ë„**: {max_complexity:.2f}",
            f"- **ìµœì†Œ ë³µì¡ë„**: {min_complexity:.2f}",
            "",
            "## ğŸ”— **ì‹¤í–‰ ìˆœì„œ (í† í´ë¡œì§€ ì •ë ¬)**",
        ]
        for index, task in enumerate(tasks):
            task_id = task.get("id")
            complexity_value = task["complexity"]
            deps_count = len(task.get("deps", []))
            headline = "{index}. **{task_id}** (ë³µì¡ë„: {complexity})".format(
                index=index + 1,
                task_id=task_id,
                complexity=complexity_value,
            )
            report_lines.extend(
                [
                    headline,
                    f"   - ì œëª©: {task.get('title', 'N/A')}",
                    f"   - íƒ€ì…: {task.get('type', 'N/A')}",
                    f"   - ëª¨ë“ˆ: {task.get('module', 'N/A')}",
                    f"   - ì˜ì¡´ì„±: {deps_count}ê°œ",
                    "",
                ]
            )
        report_lines.append("## ğŸ“‹ **íƒ€ì…ë³„ ë³µì¡ë„ ë¶„ì„**\n")
        for task_type, complexities in type_stats.items():
            avg_value = sum(complexities) / len(complexities)
            sample_count = len(complexities)
            summary = "- **{name}**: í‰ê·  {avg:.2f} (n={count})".format(
                name=task_type,
                avg=avg_value,
                count=sample_count,
            )
            report_lines.append(summary)
        report_lines.append("\n## ğŸ—ï¸ **ëª¨ë“ˆë³„ ë³µì¡ë„ ë¶„ì„**\n")
        for module, complexities in module_stats.items():
            avg_value = sum(complexities) / len(complexities)
            sample_count = len(complexities)
            summary = "- **{name}**: í‰ê·  {avg:.2f} (n={count})".format(
                name=module,
                avg=avg_value,
                count=sample_count,
            )
            report_lines.append(summary)
        report_lines.append("\n## ğŸ¯ **ë³µì¡ë„ ìˆœìœ„ (ë†’ì€ ìˆœ)**\n")
        top_tasks = sorted(
            tasks,
            key=lambda candidate: candidate["complexity"],
            reverse=True,
        )[:5]
        for index, task in enumerate(top_tasks):
            task_id = task.get("id")
            ranking_line = "{index}. **{task_id}**: {score}".format(
                index=index + 1,
                task_id=task_id,
                score=task["complexity"],
            )
            report_lines.append(ranking_line)
        completed_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        footer = f"---\n*ë¦¬í”Œë ‰ì…˜ ì™„ë£Œ: {completed_at}*"
        report_lines.extend(["", footer, ""])
        return "\n".join(report_lines)
