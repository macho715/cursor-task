"""íƒœìŠ¤í¬ ë¦¬í”Œë ‰ì…˜ ì½”ì–´ ëª¨ë“ˆ | Task reflection core module."""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Mapping, MutableMapping, Sequence, Tuple


DEFAULT_DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%S"


@dataclass(frozen=True)
class TaskSpec:
    """íƒœìŠ¤í¬ ì›ë³¸ ìŠ¤í™ | Original task specification."""

    task_id: str
    title: str = ""
    task_type: str = "code"
    dependencies: Tuple[str, ...] = field(default_factory=tuple)
    module: str = "unknown"
    extra: Mapping[str, Any] = field(default_factory=dict)

    @classmethod
    def from_mapping(cls, payload: Mapping[str, Any]) -> "TaskSpec":
        """ë§¤í•‘ìœ¼ë¡œë¶€í„° íƒœìŠ¤í¬ ìƒì„± | Build task from mapping."""

        dependencies = tuple(payload.get("deps", []))
        extra = {
            key: value
            for key, value in payload.items()
            if key not in {"id", "title", "type", "deps", "module"}
        }
        return cls(
            task_id=str(payload["id"]),
            title=str(payload.get("title", "")),
            task_type=str(payload.get("type", "code")),
            dependencies=dependencies,
            module=str(payload.get("module", "unknown")),
            extra=extra,
        )

    def to_mapping(self) -> Dict[str, Any]:
        """íƒœìŠ¤í¬ë¥¼ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜ | Convert task to mapping."""

        payload: Dict[str, Any] = {
            "id": self.task_id,
            "title": self.title,
            "type": self.task_type,
            "deps": list(self.dependencies),
            "module": self.module,
        }
        payload.update(self.extra)
        return payload


@dataclass(frozen=True)
class ReflectedTask(TaskSpec):
    """ë¦¬í”Œë ‰ì…˜ëœ íƒœìŠ¤í¬ ìŠ¤í™ | Reflected task specification."""

    order: int = 0
    complexity: float = 0.0

    def to_mapping(self) -> Dict[str, Any]:
        """ë¦¬í”Œë ‰ì…˜ ê²°ê³¼ ë§¤í•‘ ë³€í™˜ | Convert reflection result to mapping."""

        payload = super().to_mapping()
        payload.update({"order": self.order, "complexity": round(self.complexity, 2)})
        return payload


@dataclass(frozen=True)
class TaskDataset:
    """íƒœìŠ¤í¬ ë°ì´í„° ì„¸íŠ¸ | Task dataset container."""

    tasks: List[TaskSpec]
    meta: MutableMapping[str, Any] = field(default_factory=dict)
    extras: MutableMapping[str, Any] = field(default_factory=dict)

    @classmethod
    def from_mapping(cls, payload: Mapping[str, Any]) -> "TaskDataset":
        """ë§¤í•‘ìœ¼ë¡œë¶€í„° ë°ì´í„° ì„¸íŠ¸ ìƒì„± | Build dataset from mapping."""

        tasks = [TaskSpec.from_mapping(task) for task in payload.get("tasks", [])]
        meta: MutableMapping[str, Any] = dict(payload.get("meta", {}))
        extras: MutableMapping[str, Any] = {
            key: value
            for key, value in payload.items()
            if key not in {"tasks", "meta"}
        }
        return cls(tasks=tasks, meta=meta, extras=extras)

    def to_mapping(self, tasks: Sequence[ReflectedTask], meta: Mapping[str, Any]) -> Dict[str, Any]:
        """ë¦¬í”Œë ‰ì…˜ ê²°ê³¼ë¥¼ ë³‘í•© | Merge reflection results into mapping."""

        payload: Dict[str, Any] = dict(self.extras)
        payload["tasks"] = [task.to_mapping() for task in tasks]
        payload["meta"] = dict(meta)
        return payload


@dataclass(frozen=True)
class ReflectionResult:
    """ë¦¬í”Œë ‰ì…˜ ê²°ê³¼ | Reflection result container."""

    dataset: TaskDataset
    tasks: List[ReflectedTask]
    topo_order: List[str]
    cycles: List[List[str]]
    meta: Dict[str, Any]

    def to_mapping(self) -> Dict[str, Any]:
        """ë¦¬í”Œë ‰ì…˜ ê²°ê³¼ ë§¤í•‘ ë°˜í™˜ | Build mapping for reflected dataset."""

        full_meta = dict(self.dataset.meta)
        full_meta.update(self.meta)
        full_meta["topo_order"] = self.topo_order
        full_meta["cycles_detected"] = len(self.cycles)
        return self.dataset.to_mapping(self.tasks, full_meta)


class TaskReflector:
    """íƒœìŠ¤í¬ ë¦¬í”Œë ‰ì…˜ ì—”ì§„ | Task reflection engine."""

    def __init__(self) -> None:
        self.type_complexity: Mapping[str, float] = {
            "doc": 0.8,
            "cli": 0.9,
            "config": 0.9,
            "code": 1.0,
            "ide": 1.0,
            "mcp": 1.2,
            "test": 1.1,
        }
        self.title_bonus: Mapping[str, float] = {
            "complex": 0.3,
            "advanced": 0.2,
            "integration": 0.2,
            "optimization": 0.2,
            "validation": 0.1,
            "analysis": 0.1,
            "reflection": 0.1,
            "management": 0.1,
        }

    def detect_cycles(self, tasks: Sequence[TaskSpec]) -> List[List[str]]:
        """ìˆœí™˜ ì˜ì¡´ì„± íƒì§€ | Detect cyclic dependencies."""

        graph: Dict[str, Tuple[str, ...]] = {
            task.task_id: task.dependencies for task in tasks
        }
        cycles: List[List[str]] = []
        visited: set[str] = set()
        recursion_stack: set[str] = set()
        path: List[str] = []

        def depth_first(node: str) -> None:
            if node in recursion_stack:
                cycle_start = path.index(node)
                cycles.append(path[cycle_start:] + [node])
                return
            if node in visited:
                return

            visited.add(node)
            recursion_stack.add(node)
            path.append(node)

            for neighbor in graph.get(node, ()):  # type: ignore[arg-type]
                depth_first(neighbor)

            recursion_stack.remove(node)
            path.pop()

        for task_id in graph:
            if task_id not in visited:
                depth_first(task_id)

        return cycles

    def topological_sort(self, tasks: Sequence[TaskSpec]) -> Tuple[List[str], List[str]]:
        """í† í´ë¡œì§€ ì •ë ¬ ìˆ˜í–‰ | Execute topological sort."""

        in_degree: Dict[str, int] = {}
        dependents: Dict[str, List[str]] = {}

        for task in tasks:
            in_degree[task.task_id] = len(task.dependencies)
            for dependency in task.dependencies:
                dependents.setdefault(dependency, []).append(task.task_id)

        queue: List[str] = [task_id for task_id, degree in in_degree.items() if degree == 0]
        result: List[str] = []

        while queue:
            current = queue.pop(0)
            result.append(current)
            for dependent in dependents.get(current, []):
                in_degree[dependent] -= 1
                if in_degree[dependent] == 0:
                    queue.append(dependent)

        remaining = [task_id for task_id in in_degree if task_id not in result]
        return result, remaining

    def calculate_complexity(
        self, task: TaskSpec, tasks: Mapping[str, TaskSpec]
    ) -> float:
        """ë³µì¡ë„ ê³„ì‚° | Calculate task complexity."""

        base_complexity = self.type_complexity.get(task.task_type, 1.0)
        dependency_complexity = 0.2 * len(task.dependencies)
        dependents = sum(1 for candidate in tasks.values() if task.task_id in candidate.dependencies)
        dependents_complexity = 0.1 * dependents
        title_lower = task.title.lower()
        bonus = sum(
            value for keyword, value in self.title_bonus.items() if keyword in title_lower
        )
        total = base_complexity + dependency_complexity + dependents_complexity + bonus
        return max(0.8, min(3.0, total))

    def reflect_dataset(self, dataset: TaskDataset) -> ReflectionResult:
        """ë°ì´í„° ì„¸íŠ¸ ë¦¬í”Œë ‰ì…˜ | Reflect dataset."""

        cycles = self.detect_cycles(dataset.tasks)
        topo_order, remaining = self.topological_sort(dataset.tasks)

        if remaining:
            cycles.extend([[task_id] for task_id in remaining])

        task_lookup: Dict[str, TaskSpec] = {task.task_id: task for task in dataset.tasks}
        reflected_tasks: List[ReflectedTask] = []

        for order, task_id in enumerate(topo_order):
            task = task_lookup[task_id]
            complexity = self.calculate_complexity(task, task_lookup)
            reflected_tasks.append(
                ReflectedTask(
                    task_id=task.task_id,
                    title=task.title,
                    task_type=task.task_type,
                    dependencies=task.dependencies,
                    module=task.module,
                    extra=task.extra,
                    order=order,
                    complexity=round(complexity, 2),
                )
            )

        meta = {
            "reflected_at": datetime.utcnow().strftime(DEFAULT_DATETIME_FORMAT),
            "topo_order": topo_order,
            "cycles_detected": len(cycles),
        }

        return ReflectionResult(
            dataset=dataset,
            tasks=reflected_tasks,
            topo_order=topo_order,
            cycles=cycles,
            meta=meta,
        )

    def reflect_mapping(self, payload: Mapping[str, Any]) -> Dict[str, Any]:
        """ë§¤í•‘ ë¦¬í”Œë ‰ì…˜ | Reflect mapping payload."""

        dataset = TaskDataset.from_mapping(payload)
        return self.reflect_dataset(dataset).to_mapping()

    def generate_report(self, result: ReflectionResult, report_path: str) -> None:
        """ë¦¬í¬íŠ¸ ìƒì„± | Generate reflection report."""

        tasks = result.tasks
        total_tasks = len(tasks)
        average_complexity = (
            sum(task.complexity for task in tasks) / total_tasks if total_tasks else 0.0
        )
        max_complexity = max((task.complexity for task in tasks), default=0.0)
        min_complexity = min((task.complexity for task in tasks), default=0.0)

        type_stats: Dict[str, List[float]] = {}
        module_stats: Dict[str, List[float]] = {}

        for task in tasks:
            type_stats.setdefault(task.task_type, []).append(task.complexity)
            module_stats.setdefault(task.module, []).append(task.complexity)

        reflected_at = result.meta.get("reflected_at", "N/A")
        source = result.dataset.meta.get("source", "N/A")

        lines = ["# íƒœìŠ¤í¬ ë¦¬í”Œë ‰ì…˜ ë¦¬í¬íŠ¸", "", "## ğŸ“Š **ë¦¬í”Œë ‰ì…˜ ìš”ì•½**"]
        lines.append(f"- **ìƒì„±ì¼**: {reflected_at}")
        lines.append(f"- **ì›ë³¸ íŒŒì¼**: {source}")
        lines.append(f"- **íƒœìŠ¤í¬ ìˆ˜**: {total_tasks}ê°œ")
        lines.append(f"- **ìˆœí™˜ ì˜ì¡´ì„±**: {len(result.cycles)}ê°œ ë°œê²¬")
        lines.append("")

        lines.append("## ğŸ“ˆ **ë³µì¡ë„ í†µê³„**")
        lines.append(f"- **í‰ê·  ë³µì¡ë„**: {average_complexity:.2f}")
        lines.append(f"- **ìµœëŒ€ ë³µì¡ë„**: {max_complexity:.2f}")
        lines.append(f"- **ìµœì†Œ ë³µì¡ë„**: {min_complexity:.2f}")
        lines.append("")

        lines.append("## ğŸ”— **ì‹¤í–‰ ìˆœì„œ (í† í´ë¡œì§€ ì •ë ¬)**")
        for order, task in enumerate(tasks, start=1):
            lines.append(f"{order}. **{task.task_id}** (ë³µì¡ë„: {task.complexity:.2f})")
            lines.append(f"   - ì œëª©: {task.title or 'N/A'}")
            lines.append(f"   - íƒ€ì…: {task.task_type}")
            lines.append(f"   - ëª¨ë“ˆ: {task.module}")
            lines.append(f"   - ì˜ì¡´ì„±: {len(task.dependencies)}ê°œ")
            lines.append("")

        lines.append("## ğŸ“‹ **íƒ€ì…ë³„ ë³µì¡ë„ ë¶„ì„**")
        lines.append("")
        for task_type, complexities in type_stats.items():
            average = sum(complexities) / len(complexities)
            lines.append(
                f"- **{task_type}**: í‰ê·  {average:.2f} (n={len(complexities)})"
            )

        lines.append("")
        lines.append("## ğŸ—ï¸ **ëª¨ë“ˆë³„ ë³µì¡ë„ ë¶„ì„**")
        lines.append("")
        for module, complexities in module_stats.items():
            average = sum(complexities) / len(complexities)
            lines.append(
                f"- **{module}**: í‰ê·  {average:.2f} (n={len(complexities)})"
            )

        lines.append("")
        lines.append("## ğŸ¯ **ë³µì¡ë„ ìˆœìœ„ (ë†’ì€ ìˆœ)**")
        lines.append("")
        sorted_tasks = sorted(tasks, key=lambda item: item.complexity, reverse=True)
        for index, task in enumerate(sorted_tasks[:5], start=1):
            lines.append(f"{index}. **{task.task_id}**: {task.complexity:.2f}")

        lines.append("")
        lines.append(f"---\n*ë¦¬í”Œë ‰ì…˜ ì™„ë£Œ: {datetime.utcnow().strftime(DEFAULT_DATETIME_FORMAT)}*")

        with open(report_path, "w", encoding="utf-8") as file:
            file.write("\n".join(lines))


def load_dataset(filepath: str) -> TaskDataset:
    """íŒŒì¼ì—ì„œ ë°ì´í„° ì„¸íŠ¸ ë¡œë“œ | Load dataset from file."""

    import json
    import pathlib

    path = pathlib.Path(filepath)
    if not path.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
    with path.open("r", encoding="utf-8") as handle:
        payload = json.load(handle)
    dataset = TaskDataset.from_mapping(payload)
    dataset.meta.setdefault("source", str(filepath))
    return dataset


__all__ = [
    "TaskDataset",
    "TaskReflector",
    "TaskSpec",
    "ReflectedTask",
    "ReflectionResult",
    "load_dataset",
]

