#!/usr/bin/env python3
"""
ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ ì—”ì§„
ì˜ì¡´ì„± ê·¸ë˜í”„ ë¶„ì„ì„ í†µí•œ ìµœì í™”ëœ ë³‘ë ¬ ì‹¤í–‰

ì‚¬ìš©ë²•:
    python tools/parallel_executor.py --input tasks.reflected.json --max-workers 4
    python tools/parallel_executor.py --input tasks.reflected.json --strategy smart --visualize
"""

import json
import argparse
import subprocess
import sys
import time
import logging
import threading
import queue
from datetime import datetime
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass, field
from enum import Enum
import concurrent.futures
import networkx as nx
from collections import defaultdict, deque

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parallel_execution.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ParallelStrategy(Enum):
    """ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ"""
    CONSERVATIVE = "conservative"  # ì•ˆì „í•œ ë³‘ë ¬ ì²˜ë¦¬
    AGGRESSIVE = "aggressive"      # ê³µê²©ì ì¸ ë³‘ë ¬ ì²˜ë¦¬
    SMART = "smart"               # ì§€ëŠ¥ì  ë³‘ë ¬ ì²˜ë¦¬
    DEPENDENCY = "dependency"      # ì˜ì¡´ì„± ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬

@dataclass
class Task:
    """íƒœìŠ¤í¬ ì •ë³´"""
    id: str
    title: str
    module: str
    type: str
    deps: List[str]
    complexity: float
    acceptance: List[str]
    order: int
    
    # ë³‘ë ¬ ì²˜ë¦¬ ê´€ë ¨ í•„ë“œ
    _can_parallel: bool = True
    estimated_duration: float = 0.0
    priority_score: float = 0.0
    resource_requirements: Dict[str, int] = field(default_factory=dict)
    
    @property
    def can_parallel(self) -> bool:
        """ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ì„±"""
        parallel_types = ['config', 'cli', 'doc', 'test']
        sequential_types = ['code', 'ide', 'mcp']
        
        if self.type in parallel_types:
            return True
        elif self.type in sequential_types:
            return False
        else:
            return False
    
    @property
    def is_ready(self) -> bool:
        """ì˜ì¡´ì„±ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return len(self.deps) == 0 or all(dep in ParallelExecutor.completed_tasks for dep in self.deps)
    
    @property
    def parallel_compatibility(self) -> str:
        """ë³‘ë ¬ í˜¸í™˜ì„± ë ˆë²¨"""
        if self.can_parallel:
            if self.type in ['config', 'cli', 'doc']:
                return "high"
            elif self.type in ['test']:
                return "medium"
            else:
                return "medium"
        else:
            return "low"

@dataclass
class ExecutionResult:
    """ì‹¤í–‰ ê²°ê³¼"""
    task_id: str
    success: bool
    duration: float
    output: str
    error: Optional[str] = None
    timestamp: str = ""
    worker_id: Optional[int] = None
    parallel_group: Optional[str] = None

@dataclass
class ParallelGroup:
    """ë³‘ë ¬ ì‹¤í–‰ ê·¸ë£¹"""
    id: str
    tasks: List[Task]
    estimated_duration: float
    resource_usage: Dict[str, int]
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None

class ParallelExecutor:
    """ë³‘ë ¬ ì‹¤í–‰ê¸°"""
    
    completed_tasks = set()
    failed_tasks = set()
    
    def __init__(self, strategy: ParallelStrategy = ParallelStrategy.SMART, max_workers: int = 4):
        self.strategy = strategy
        self.max_workers = max_workers
        self.results: List[ExecutionResult] = []
        self.parallel_groups: List[ParallelGroup] = []
        self.dependency_graph = nx.DiGraph()
        self.resource_pool = defaultdict(int)
        self.worker_pool = queue.Queue()
        
        # ì›Œì»¤ í’€ ì´ˆê¸°í™”
        for i in range(max_workers):
            self.worker_pool.put(i)
    
    def load_tasks(self, input_file: str) -> List[Task]:
        """íƒœìŠ¤í¬ íŒŒì¼ ë¡œë“œ ë° ë¶„ì„"""
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            tasks = []
            for task_data in data.get('tasks', []):
                task = Task(
                    id=task_data['id'],
                    title=task_data['title'],
                    module=task_data['module'],
                    type=task_data['type'],
                    deps=task_data.get('deps', []),
                    complexity=task_data.get('complexity', 1.0),
                    acceptance=task_data.get('acceptance', []),
                    order=task_data.get('order', 0)
                )
                
                # ë³‘ë ¬ ì²˜ë¦¬ ê´€ë ¨ ì†ì„± ì„¤ì •
                task.estimated_duration = self._estimate_duration(task)
                task.priority_score = self._calculate_priority(task)
                task.resource_requirements = self._get_resource_requirements(task)
                
                tasks.append(task)
            
            logger.info(f"íƒœìŠ¤í¬ {len(tasks)}ê°œ ë¡œë“œ ì™„ë£Œ")
            return tasks
            
        except Exception as e:
            logger.error(f"íƒœìŠ¤í¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    
    def _estimate_duration(self, task: Task) -> float:
        """íƒœìŠ¤í¬ ì‹¤í–‰ ì‹œê°„ ì¶”ì •"""
        # ë³µì¡ë„ ê¸°ë°˜ ì‹œê°„ ì¶”ì • (ì´ˆ)
        base_time = 2.0
        complexity_factor = task.complexity
        type_factor = {
            'code': 1.5,
            'config': 0.8,
            'cli': 1.0,
            'mcp': 2.0,
            'ide': 1.2,
            'test': 1.0,
            'doc': 0.5
        }.get(task.type, 1.0)
        
        return base_time * complexity_factor * type_factor
    
    def _calculate_priority(self, task: Task) -> float:
        """íƒœìŠ¤í¬ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
        # ë³µì¡ë„ê°€ ë†’ì„ìˆ˜ë¡, ì˜ì¡´ì„±ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„
        complexity_score = task.complexity * 0.4
        dependency_score = (10 - len(task.deps)) * 0.3
        type_score = {
            'code': 0.3,
            'config': 0.2,
            'cli': 0.25,
            'mcp': 0.35,
            'ide': 0.3,
            'test': 0.2,
            'doc': 0.1
        }.get(task.type, 0.2)
        
        return complexity_score + dependency_score + type_score
    
    def _get_resource_requirements(self, task: Task) -> Dict[str, int]:
        """íƒœìŠ¤í¬ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­"""
        return {
            'cpu': 1 if task.type in ['code', 'mcp'] else 0.5,
            'memory': 512 if task.type in ['code', 'mcp'] else 256,
            'disk': 100 if task.type in ['doc', 'test'] else 50
        }
    
    def build_dependency_graph(self, tasks: List[Task]) -> nx.DiGraph:
        """ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•"""
        graph = nx.DiGraph()
        
        # ë…¸ë“œ ì¶”ê°€
        for task in tasks:
            graph.add_node(task.id, task=task)
        
        # ì—£ì§€ ì¶”ê°€ (ì˜ì¡´ì„±)
        for task in tasks:
            for dep in task.deps:
                graph.add_edge(dep, task.id)
        
        self.dependency_graph = graph
        logger.info(f"ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ: {graph.number_of_nodes()}ê°œ ë…¸ë“œ, {graph.number_of_edges()}ê°œ ì—£ì§€")
        
        return graph
    
    def identify_parallel_groups(self, tasks: List[Task]) -> List[ParallelGroup]:
        """ë³‘ë ¬ ì‹¤í–‰ ê·¸ë£¹ ì‹ë³„"""
        groups = []
        visited = set()
        
        # ì˜ì¡´ì„± ë ˆë²¨ë³„ë¡œ ê·¸ë£¹í™”
        levels = self._get_dependency_levels(tasks)
        
        for level, level_tasks in levels.items():
            if len(level_tasks) > 1:
                # ê°™ì€ ë ˆë²¨ì˜ íƒœìŠ¤í¬ë“¤ì„ ë³‘ë ¬ ê·¸ë£¹ìœ¼ë¡œ êµ¬ì„±
                parallel_tasks = [t for t in level_tasks if t.can_parallel]
                sequential_tasks = [t for t in level_tasks if not t.can_parallel]
                
                if parallel_tasks:
                    group = ParallelGroup(
                        id=f"level_{level}_parallel",
                        tasks=parallel_tasks,
                        estimated_duration=max(t.estimated_duration for t in parallel_tasks),
                        resource_usage=self._calculate_group_resources(parallel_tasks)
                    )
                    groups.append(group)
                
                # ìˆœì°¨ ì‹¤í–‰ íƒœìŠ¤í¬ë“¤ë„ ê°œë³„ ê·¸ë£¹ìœ¼ë¡œ ì¶”ê°€
                for task in sequential_tasks:
                    group = ParallelGroup(
                        id=f"sequential_{task.id}",
                        tasks=[task],
                        estimated_duration=task.estimated_duration,
                        resource_usage=task.resource_requirements
                    )
                    groups.append(group)
            else:
                # ë‹¨ì¼ íƒœìŠ¤í¬ ê·¸ë£¹
                task = level_tasks[0]
                group = ParallelGroup(
                    id=f"single_{task.id}",
                    tasks=[task],
                    estimated_duration=task.estimated_duration,
                    resource_usage=task.resource_requirements
                )
                groups.append(group)
        
        self.parallel_groups = groups
        logger.info(f"ë³‘ë ¬ ê·¸ë£¹ {len(groups)}ê°œ ì‹ë³„ ì™„ë£Œ")
        
        return groups
    
    def _get_dependency_levels(self, tasks: List[Task]) -> Dict[int, List[Task]]:
        """ì˜ì¡´ì„± ë ˆë²¨ë³„ íƒœìŠ¤í¬ ë¶„ë¥˜"""
        levels = defaultdict(list)
        task_dict = {task.id: task for task in tasks}
        
        # ì˜ì¡´ì„± ê¹Šì´ë³„ë¡œ ë ˆë²¨ ê³„ì‚°
        def get_dependency_depth(task_id: str, visited: set = None) -> int:
            if visited is None:
                visited = set()
            
            if task_id in visited:
                return 0  # ìˆœí™˜ ì˜ì¡´ì„± ë°©ì§€
            
            visited.add(task_id)
            task = task_dict[task_id]
            
            if not task.deps:
                return 0
            
            max_depth = max(get_dependency_depth(dep, visited.copy()) for dep in task.deps)
            return max_depth + 1
        
        # ê° íƒœìŠ¤í¬ì˜ ì˜ì¡´ì„± ê¹Šì´ ê³„ì‚°
        for task in tasks:
            depth = get_dependency_depth(task.id)
            levels[depth].append(task)
        
        return dict(levels)
    
    def _calculate_group_resources(self, tasks: List[Task]) -> Dict[str, int]:
        """ê·¸ë£¹ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        total_resources = defaultdict(int)
        for task in tasks:
            for resource, amount in task.resource_requirements.items():
                total_resources[resource] += amount
        return dict(total_resources)
    
    def execute_parallel_groups(self, groups: List[ParallelGroup]) -> List[ExecutionResult]:
        """ë³‘ë ¬ ê·¸ë£¹ ì‹¤í–‰"""
        logger.info(f"ë³‘ë ¬ ê·¸ë£¹ ì‹¤í–‰ ì‹œì‘ (ìµœëŒ€ {self.max_workers}ê°œ ì›Œì»¤)")
        
        all_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ê·¸ë£¹ë³„ë¡œ ì‹¤í–‰
            for group in groups:
                group.start_time = datetime.now()
                logger.info(f"ê·¸ë£¹ {group.id} ì‹¤í–‰ ì‹œì‘ ({len(group.tasks)}ê°œ íƒœìŠ¤í¬)")
                
                # ê·¸ë£¹ ë‚´ íƒœìŠ¤í¬ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                if len(group.tasks) == 1:
                    # ë‹¨ì¼ íƒœìŠ¤í¬
                    task = group.tasks[0]
                    result = self.execute_task(task, worker_id=0)
                    all_results.append(result)
                else:
                    # ë‹¤ì¤‘ íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰
                    future_to_task = {}
                    for i, task in enumerate(group.tasks):
                        if task.is_ready:
                            future = executor.submit(self.execute_task, task, worker_id=i % self.max_workers)
                            future_to_task[future] = task
                    
                    # ê²°ê³¼ ìˆ˜ì§‘
                    for future in concurrent.futures.as_completed(future_to_task):
                        task = future_to_task[future]
                        try:
                            result = future.result()
                            all_results.append(result)
                            
                            if not result.success:
                                logger.error(f"íƒœìŠ¤í¬ {task.id} ì‹¤íŒ¨ë¡œ ì¸í•œ ê·¸ë£¹ ì‹¤í–‰ ì¤‘ë‹¨")
                                break
                        
                        except Exception as e:
                            logger.error(f"íƒœìŠ¤í¬ {task.id} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}")
                            result = ExecutionResult(
                                task_id=task.id,
                                success=False,
                                duration=0.0,
                                output="",
                                error=str(e),
                                timestamp=datetime.now().isoformat()
                            )
                            all_results.append(result)
                
                group.end_time = datetime.now()
                group_duration = (group.end_time - group.start_time).total_seconds()
                logger.info(f"ê·¸ë£¹ {group.id} ì™„ë£Œ ({group_duration:.2f}ì´ˆ)")
        
        self.results = all_results
        return all_results
    
    def execute_task(self, task: Task, worker_id: int = 0) -> ExecutionResult:
        """ë‹¨ì¼ íƒœìŠ¤í¬ ì‹¤í–‰"""
        start_time = time.time()
        timestamp = datetime.now().isoformat()
        
        logger.info(f"[ì›Œì»¤ {worker_id}] íƒœìŠ¤í¬ ì‹¤í–‰ ì‹œì‘: {task.id} ({task.title})")
        
        try:
            # íƒœìŠ¤í¬ íƒ€ì…ì— ë”°ë¥¸ ëª…ë ¹ì–´ ìƒì„±
            command = self._generate_command(task)
            
            # ëª…ë ¹ì–´ ì‹¤í–‰
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                logger.info(f"[ì›Œì»¤ {worker_id}] íƒœìŠ¤í¬ ì„±ê³µ: {task.id} ({duration:.2f}ì´ˆ)")
                self.completed_tasks.add(task.id)
                
                return ExecutionResult(
                    task_id=task.id,
                    success=True,
                    duration=duration,
                    output=result.stdout,
                    timestamp=timestamp,
                    worker_id=worker_id
                )
            else:
                logger.error(f"[ì›Œì»¤ {worker_id}] íƒœìŠ¤í¬ ì‹¤íŒ¨: {task.id} - {result.stderr}")
                self.failed_tasks.add(task.id)
                
                return ExecutionResult(
                    task_id=task.id,
                    success=False,
                    duration=duration,
                    output=result.stdout,
                    error=result.stderr,
                    timestamp=timestamp,
                    worker_id=worker_id
                )
                
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            logger.error(f"[ì›Œì»¤ {worker_id}] íƒœìŠ¤í¬ íƒ€ì„ì•„ì›ƒ: {task.id}")
            self.failed_tasks.add(task.id)
            
            return ExecutionResult(
                task_id=task.id,
                success=False,
                duration=duration,
                output="",
                error="Task execution timeout (5 minutes)",
                timestamp=timestamp,
                worker_id=worker_id
            )
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"[ì›Œì»¤ {worker_id}] íƒœìŠ¤í¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {task.id} - {e}")
            self.failed_tasks.add(task.id)
            
            return ExecutionResult(
                task_id=task.id,
                success=False,
                duration=duration,
                output="",
                error=str(e),
                timestamp=timestamp,
                worker_id=worker_id
            )
    
    def _generate_command(self, task: Task) -> str:
        """íƒœìŠ¤í¬ íƒ€ì…ì— ë”°ë¥¸ ëª…ë ¹ì–´ ìƒì„±"""
        if task.type == "code":
            return f'cursor agent --apply=ask --rules "{task.title}"'
        
        elif task.type == "config":
            return f'cursor agent --apply=ask --rules "{task.title}"'
        
        elif task.type == "cli":
            return f'cursor agent --apply=ask --rules "{task.title}"'
        
        elif task.type == "mcp":
            return f'python tools/tasks_reflect.py --in tasks.json --out tasks.reflected.json --report reports/tasks_reflect_report.md'
        
        elif task.type == "ide":
            return f'cursor agent --apply=ask --rules "{task.title}"'
        
        else:
            return f'echo "Unknown task type: {task.type}"'
    
    def generate_parallel_report(self, results: List[ExecutionResult]) -> str:
        """ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        total_tasks = len(results)
        successful_tasks = len([r for r in results if r.success])
        failed_tasks = total_tasks - successful_tasks
        
        total_duration = sum(r.duration for r in results)
        max_duration = max(r.duration for r in results) if results else 0
        min_duration = min(r.duration for r in results) if results else 0
        avg_duration = total_duration / total_tasks if total_tasks > 0 else 0
        
        # ì›Œì»¤ë³„ í†µê³„
        worker_stats = defaultdict(lambda: {'count': 0, 'duration': 0.0, 'success': 0})
        for result in results:
            worker_id = result.worker_id or 0
            worker_stats[worker_id]['count'] += 1
            worker_stats[worker_id]['duration'] += result.duration
            if result.success:
                worker_stats[worker_id]['success'] += 1
        
        # ë³‘ë ¬ ê·¸ë£¹ë³„ í†µê³„
        group_stats = []
        for group in self.parallel_groups:
            if group.start_time and group.end_time:
                group_duration = (group.end_time - group.start_time).total_seconds()
                group_stats.append({
                    'id': group.id,
                    'tasks': len(group.tasks),
                    'duration': group_duration,
                    'parallel_efficiency': group.estimated_duration / group_duration if group_duration > 0 else 0
                })
        
        report = f"""
# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë¦¬í¬íŠ¸

## ğŸ“Š **ì‹¤í–‰ ìš”ì•½**
- **ì‹¤í–‰ ì „ëµ**: {self.strategy.value}
- **ìµœëŒ€ ì›Œì»¤ ìˆ˜**: {self.max_workers}
- **ì´ íƒœìŠ¤í¬ ìˆ˜**: {total_tasks}
- **ì„±ê³µí•œ íƒœìŠ¤í¬**: {successful_tasks}
- **ì‹¤íŒ¨í•œ íƒœìŠ¤í¬**: {failed_tasks}
- **ì„±ê³µë¥ **: {(successful_tasks/total_tasks*100):.1f}%
- **ì´ ì‹¤í–‰ ì‹œê°„**: {total_duration:.2f}ì´ˆ
- **ìµœëŒ€ ì‹¤í–‰ ì‹œê°„**: {max_duration:.2f}ì´ˆ
- **ìµœì†Œ ì‹¤í–‰ ì‹œê°„**: {min_duration:.2f}ì´ˆ
- **í‰ê·  ì‹¤í–‰ ì‹œê°„**: {avg_duration:.2f}ì´ˆ

## ğŸ”§ **ì›Œì»¤ë³„ ì„±ëŠ¥**
"""
        
        for worker_id, stats in worker_stats.items():
            success_rate = (stats['success'] / stats['count'] * 100) if stats['count'] > 0 else 0
            avg_worker_duration = stats['duration'] / stats['count'] if stats['count'] > 0 else 0
            report += f"""
### ì›Œì»¤ {worker_id}
- **ì²˜ë¦¬í•œ íƒœìŠ¤í¬**: {stats['count']}ê°œ
- **ì´ ì‹¤í–‰ ì‹œê°„**: {stats['duration']:.2f}ì´ˆ
- **í‰ê·  ì‹¤í–‰ ì‹œê°„**: {avg_worker_duration:.2f}ì´ˆ
- **ì„±ê³µë¥ **: {success_rate:.1f}%
"""
        
        report += f"""
## ğŸš€ **ë³‘ë ¬ ê·¸ë£¹ë³„ ì„±ëŠ¥**
"""
        
        for group_stat in group_stats:
            efficiency = group_stat['parallel_efficiency']
            report += f"""
### {group_stat['id']}
- **íƒœìŠ¤í¬ ìˆ˜**: {group_stat['tasks']}ê°œ
- **ì‹¤í–‰ ì‹œê°„**: {group_stat['duration']:.2f}ì´ˆ
- **ë³‘ë ¬ íš¨ìœ¨ì„±**: {efficiency:.2f}x
"""
        
        report += f"""
## ğŸ“‹ **íƒœìŠ¤í¬ë³„ ê²°ê³¼**
"""
        
        for result in results:
            status = "âœ… ì„±ê³µ" if result.success else "âŒ ì‹¤íŒ¨"
            worker_info = f" (ì›Œì»¤ {result.worker_id})" if result.worker_id is not None else ""
            report += f"""
### {result.task_id}{worker_info}
- **ìƒíƒœ**: {status}
- **ì‹¤í–‰ ì‹œê°„**: {result.duration:.2f}ì´ˆ
- **íƒ€ì„ìŠ¤íƒ¬í”„**: {result.timestamp}
"""
            
            if result.error:
                report += f"- **ì˜¤ë¥˜**: {result.error}\n"
        
        if failed_tasks > 0:
            report += f"""
## âš ï¸ **ì‹¤íŒ¨í•œ íƒœìŠ¤í¬ ë¶„ì„**
"""
            for result in results:
                if not result.success:
                    report += f"- **{result.task_id}**: {result.error}\n"
        
        # ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„± ë¶„ì„
        sequential_time = sum(t.estimated_duration for t in self._get_all_tasks())
        parallel_time = max(group_stat['duration'] for group_stat in group_stats) if group_stats else 0
        speedup = sequential_time / parallel_time if parallel_time > 0 else 0
        
        report += f"""
## ğŸ¯ **ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„±**
- **ìˆœì°¨ ì‹¤í–‰ ì˜ˆìƒ ì‹œê°„**: {sequential_time:.2f}ì´ˆ
- **ë³‘ë ¬ ì‹¤í–‰ ì‹¤ì œ ì‹œê°„**: {parallel_time:.2f}ì´ˆ
- **ì†ë„ í–¥ìƒ**: {speedup:.2f}x
- **íš¨ìœ¨ì„±**: {(speedup/self.max_workers*100):.1f}%

## ğŸ’¡ **ê°œì„  ì œì•ˆ**
"""
        
        if speedup < self.max_workers * 0.7:
            report += "- ì›Œì»¤ ìˆ˜ë¥¼ ì¡°ì •í•˜ê±°ë‚˜ íƒœìŠ¤í¬ ë¶„í• ì„ ì¬ê²€í† í•˜ì„¸ìš”\n"
        
        if failed_tasks > 0:
            report += "- ì‹¤íŒ¨í•œ íƒœìŠ¤í¬ì˜ ì˜ì¡´ì„± ë° ì„¤ì •ì„ ì¬ê²€í† í•˜ì„¸ìš”\n"
        
        if any(stat['success'] / stat['count'] < 0.8 for stat in worker_stats.values()):
            report += "- ì›Œì»¤ë³„ ì„±ê³µë¥ ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë¦¬ì†ŒìŠ¤ í• ë‹¹ì„ ìµœì í™”í•˜ì„¸ìš”\n"
        
        return report
    
    def _get_all_tasks(self) -> List[Task]:
        """ëª¨ë“  íƒœìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°"""
        tasks = []
        for group in self.parallel_groups:
            tasks.extend(group.tasks)
        return tasks
    
    def visualize_dependency_graph(self, output_file: str = "dependency_graph.png"):
        """ì˜ì¡´ì„± ê·¸ë˜í”„ ì‹œê°í™”"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib.patches as mpatches
            
            plt.figure(figsize=(12, 8))
            pos = nx.spring_layout(self.dependency_graph, k=3, iterations=50)
            
            # ë…¸ë“œ ìƒ‰ìƒ ì„¤ì • (íƒœìŠ¤í¬ íƒ€ì…ë³„)
            node_colors = []
            color_map = {
                'code': 'red',
                'config': 'blue',
                'cli': 'green',
                'mcp': 'orange',
                'ide': 'purple',
                'test': 'brown',
                'doc': 'pink'
            }
            
            for node in self.dependency_graph.nodes():
                task = self.dependency_graph.nodes[node]['task']
                node_colors.append(color_map.get(task.type, 'gray'))
            
            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            nx.draw(self.dependency_graph, pos, 
                   node_color=node_colors,
                   node_size=1000,
                   font_size=8,
                   font_weight='bold',
                   with_labels=True,
                   arrows=True,
                   arrowsize=20,
                   edge_color='gray')
            
            # ë²”ë¡€ ì¶”ê°€
            legend_elements = [mpatches.Patch(color=color, label=task_type) 
                             for task_type, color in color_map.items()]
            plt.legend(handles=legend_elements, loc='upper right')
            
            plt.title("Task Dependency Graph", fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ì˜ì¡´ì„± ê·¸ë˜í”„ ì‹œê°í™” ì™„ë£Œ: {output_file}")
            
        except ImportError:
            logger.warning("matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë³‘ë ¬ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹¤í–‰')
    parser.add_argument('--input', '-i', required=True, help='ì…ë ¥ íƒœìŠ¤í¬ íŒŒì¼')
    parser.add_argument('--strategy', '-s', choices=['conservative', 'aggressive', 'smart', 'dependency'], 
                       default='smart', help='ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ')
    parser.add_argument('--max-workers', '-w', type=int, default=4, help='ìµœëŒ€ ì›Œì»¤ ìˆ˜')
    parser.add_argument('--output', '-o', help='ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--visualize', '-v', action='store_true', help='ì˜ì¡´ì„± ê·¸ë˜í”„ ì‹œê°í™”')
    
    args = parser.parse_args()
    
    try:
        # ì‹¤í–‰ê¸° ì´ˆê¸°í™”
        strategy = ParallelStrategy(args.strategy)
        executor = ParallelExecutor(strategy, args.max_workers)
        
        # íƒœìŠ¤í¬ ë¡œë“œ
        tasks = executor.load_tasks(args.input)
        
        # ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
        executor.build_dependency_graph(tasks)
        
        # ì‹œê°í™” (ì˜µì…˜)
        if args.visualize:
            executor.visualize_dependency_graph()
        
        # ë³‘ë ¬ ê·¸ë£¹ ì‹ë³„
        groups = executor.identify_parallel_groups(tasks)
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = executor.execute_parallel_groups(groups)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = executor.generate_parallel_report(results)
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ë¦¬í¬íŠ¸ ì €ì¥: {args.output}")
        else:
            print(report)
        
        # ì„±ê³µ/ì‹¤íŒ¨ ì¢…ë£Œ ì½”ë“œ
        failed_count = len([r for r in results if not r.success])
        sys.exit(failed_count)
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
